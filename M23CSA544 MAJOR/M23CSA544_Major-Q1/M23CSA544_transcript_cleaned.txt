 All right, last class, what did we discuss? Windowing mechanisms. Windowing mechanisms, right? Now, windowing mechanisms are applicable for all kinds of tasks there are, right? Because you have larger audio signals, you have to break it down into parts, parts, parts in order to process them, right? So, windowing, curling, and now let's look at what are the actual tasks that we can perform. Speech enhancement was one of the topics also that somebody listed, right? So, what is speech enhancement? Very simple, right? You have any audio signal can have different kinds of noises. So, speech enhancement talks about cleaning the speech, removing the noise, while ensuring that the actual content is preserved. That is not changed, right? Now, there are different kinds of noises and different kinds of noise enhancement techniques that are associated with it. The first one is you have just any kind of noise to it. For example, it's a Gaussian noise, it's a white noise, any kind of noise. The only task is just to remove that noise, right? And preserve the audio signal. So, that is that one very basic sort of an operation. Remove noise signals from audio. Remove noise from audio signals, right? The second one is signals for separation. What is signals for separation? The signals coming from multiple sources. So, you have to separate out those signals. Those multiple sources could be multiple people speaking. One person speaking, another is noise. One person speaking, other is music. It could be any of those sort of things. So, signal source separation. What is the third one? Speech enhancement. Speech enhancement looks very similar to what we earlier talked about. So, what is the difference? In this, what we try to do is two things that we want to ensure. First, the noise should be less. Second, the words that were spoken, the text that was spoken, its legibility should not be lost. What do we mean by legibility? It's clarity, the clarity of the audio should not be lost. One could be that I removed all the noise. That is one metric. The noise ratio of the signal is that I removed all the noise. But the content of the signal that was distorted. So, when I am talking about speech enhancement here as a third point, one of the metrics that we also need to ensure is legibility should be preserved. The audio content should not be distorted at all. If your noise is of those specific frequencies, lower, higher, whatever it is, if your noise is of very specific frequencies, then you can apply those kinds of filters, which just removes that. But if the noise is more complicated than that, if it's amusing, if noise is more garbled and more complicated, then the simple filtering, and then we have speech de-reverbation. What is speech de-reverbation? I'll go into the details in the next slide. So, when we talk about removing noise from audio signals, what do we do? If you look at the mathematical formulation, you have that Xt is the original signal, time domain name, NT is the noise, actually any of these noises that we have listed. And alpha is what? Alpha is the scaling factor, which controls the strength of the noise, keeping noises in the input signal. So, you have this additive noise that is there. It's nothing but noise plus, audio signal plus a scaled version of the noise with the corresponding to that alpha. And then you have different kinds of noises, white noise is your random signal, Gaussian noise is normally distributed, ambient noise, impulse noise. So, if you have that ambient noise, Gaussian noise, it's okay. That normal filtering might just work. But if you have more of an ambient noise, crowd chatter, traffic, then you would have chatter from the background, honking of the car, or any such kinds of things. So, removing that would be a little interesting. Then you have speech enhancement, like I said, speech should not be degraded. So, the legibility or the way it is called is intelligibility and quality of the speech should not be degraded by noise. There is signal source separation. The first one is showing you a constant signal, a spectrum of a constant signal. The second one is noise. What you actually get is the third one. What you actually get is that Yt. From this Yt, you have to extract out Xt and the noise. The predicted version as we used to call in the estimation, when we used to read MLE, maximum likelihood estimate, everyone has read it. So, when you read MLE, you denote that. So, X cap t is what I'm interested in extracting out of it. Somebody who is working on watermark, this NT will be this. NT will be your watermark. And in that case, you are not only interested in X cap t, but you are also interested in N cap t, the estimated value of the watermark, which is equivalent to noise for me right now. And what will be, how will you scale it in that case? Somebody who is doing watermarking, actually. The scaling factor will come in depending on what is the amplitude or what is the loudness of that audio. How loud is the noise or how loud is the signal? Because you want the final audio, may noise should not be audible. Again, I'm going back to watermarking. The noise that you have added in the watermark, that should not be audible. It is imperceptible. So that people cannot decipher that copyrighted content is added somewhere. So, depending on the strength of the audio, energy of the original audio, the energy of the watermark needs to be adapted. So that if the audio is too low, you cannot add drum beats in it. So, you have to scale it down corresponding to or adapted corresponding to the original. So, this is your signal source separation. When you are adding noise, you are doing watermarking, then this will do. When you are denoising or dewatermarking, you are doing watermark extraction, then you will do watermark. Then speech debubberation is what is it? Let's say we are speaking in a closed room, in a large closed room. Take a look at that kind of a room. So, nowadays this is what happens. So, at my home, I have a comparatively sized living room. Or I keep a TV on, where people can sit and watch TV. So, sometimes there is a lot of echo that happens. Sometimes there isn't. I still have to figure out why this happens. So, if somebody is playing a bhajan on YouTube, it's very clear or some movie on YouTube, it's very clear. Whereas if you are or any kind of video, but sometimes it is very, very there is too much of echo that is happening. And I cannot hear anything, cannot decipher any kind of text that is any kind of audio that is being spoken. That is what it is called about the works. So, when you have this echo effect, with every word being spoken, there is echo and then it fades down. So, you can hear, right? Then it slowly, slowly, slowly fades out. But as it is fading out, there is signal coming back to you, which is adding to the audio that you are actually hearing. So, there are multiple audio signals that are coming back for every spoken word that you heard. And then you actually hear it back. So, kya sunte hua? XT sunoge? Plus XT minus 1 ka audio bapas aaya echo ke karan, wo add hua. XT minus 2 ka further faded audio aaya, wo sunoge. XT minus 3 ka further faded audio aaya, wo sunoge. Everything gets added to the audio and you hear a combined voice. So, sometimes you are not able to hear it clearly. So, this is what is called as this noise. Now, what is the expectation here? Again, to do the signal source separation. You have to separate out the original signal and you have to separate out the echo. Correct. Kyo ki ussi cheez ki whatever you had spoken, ussi cheez ki echo bapas aaya. Time lag hoga. So, depending on the environment that is there and what speed it is coming back and what does the fading out meh cancel. So, based on that, you have to figure out and extract ki decay factor kya tha. So, wo noise kya hai, jo reverberation factor kya hai, actual signal kya hai and then extract from there. So, your direct path of the sound from the source to the receiver, whereas this reflection that you are getting from the echoes, this contributes to the ith reflection of sound waves off of different kinds of surfaces that are there. So, that is your reverberation noise. And what happens here is like I was saying that clarity of words is lost. So, if you look at the original audio signal, uska spectrogram, you can see clear words. You can see clear separation in the audio spectrogram. But when you see actually the spectrogram of the reverberation audio that the final audio that was actually captured, clarity lost. So, if the clarity is lost, then it becomes difficult to understand and any of the TTS, speaker recognition, speech recognition, whatever we have been talking about that makes our life difficult. So, you have to figure out what kind of noise is there and then figure out mechanisms of extracting that. So, these are the different kinds of noises that are mechanisms of different kinds of noises that are added. And a very, very interesting problem that has been derived out of this, it's called as a cocktail party problem. What does it mean? If you are in a cocktail party, and it could be your whatever party you have, Iglis the party or Sumit Yuga party, the ability to focus on once listening, attention on the single talker among the catapult of conversations and background numbers. Because there is a lot of noise in the party. Interestingly, if you add reverberation to it, we like our making our lives interesting, so add reverberations to it. At that point in time, focusing on a single audio and doing this source separation. I want to focus on, there are a lot of students talking, like Ravi was talking, so I have to extract out everybody's audio and focus on what every student is saying. For example, I'm talking right now, right? Few minutes earlier, everybody was in the class was giving answers at a time. Students were sitting online. I'm not sure how much they can hear when you guys are speaking. Because I'm standing here, I'm sure they can hear me. But I have a lot of doubt in that. So that is why I was checking the broadcast. So, in the next class, we can use that mic. So, in the next class, when you guys are speaking, I can use the mic so that even students who are online, they can clearly hear what you are saying. So, cocktail party, right? That is an interesting thing. So, motivation to build a system that can simultaneously present multiple streams of speech, that the user can focus on one and easily shift attention to others rather than getting distracted while teaching. There are these... Apple's glasses... What do you call them? Vision Pro. No, not Apple's, sorry, sorry, sorry. Meta's glasses. Meta's glasses. Oculus? Meta's glasses. It got launched two years back. They have collaborated with Redan and created these research glasses. What's the name? Okay, I'll tell you the name in the next class. I'm forgetting. I have those. I'm forgetting the name. So, there is a lot of gadgets in that. There are cameras. There are... He's searching for the glasses. There are cameras in that. There are... There's microphone in that. There is audio card connection in that. A lot of gadgets in that. 3D light, what is in those things. So, what they want to do is one of the tasks that can be done, and that is something that some of the students with me are working there. There are two B. Techs, a Kenya student is working. There are some PhD students working on that problem. What they are trying to do is can we identify, do these kinds of signal source separation, and help the speaker focus on somebody else's voice? So, you can actually... If I'm very interested in just talking to you, I'm at a cocktail party, right? And I'm just interested in talking to you, can I configure it to blurt out everything else? Blurt out everything else and just focus on that audio. I think this is what those earpods are sometimes doing. So, I keep giving examples. I still have to do this testing because I was telling you about those earpods in one of the classes. So, now I started using those. Now, after using those, so evening I generally go for a walk around 8 or so when I go back home. So, 8, 9 o'clock I go for a walk, and there is a marriage hall very close to my home. And this is wedding season. So, there are a lot of songs playing. Although I used to get very disturbed, but after using those, I faintly hear that the wedding songs are playing. So, it is obviously it is blurting out the background noise a lot for me. So, I have to figure that out what is happening, how is it happening. But this is what is your cocktail party problem. So, you could have a variety of signals and extract it, extract this out. So, this is an analogy in the image domain. This is for all of these because we understand image domain better. This is the thing that we are talking about. This is not an analogy for cocktail party. This is an analogy just for enhancement. That you can have a blurry image. You want to extract out original and the noise from that original image. It is more than one right now. You will have next class. So, I will stop here today. And next class I will go into what are the evaluation metrics for some of these tasks. And some algorithms of how these individual tasks, these denoting has been achieved in the literature. Some of the tasks that are there in the literature. Some of the algorithms that are there in the literature. So, I will stop here guys. Please make sure to complete the assignment. Submit it by Sunday, coming Sunday. And have the presentation in place before Monday's class. Alright. Thank you. I will see you on Monday. Students online, any questions? Just one quick question regarding that reverberation slide. You showed a formula for that. In that, I think... So, why is like Xt multiplied with ht? I mean, what is like the expanded form? Does it mean... So, why is Xt multiplied by? Because what is... See, this is written in the Fourier domain right now. So, you can have it like that. Anything else? Anyone?